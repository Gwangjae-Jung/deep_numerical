{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from    typing              import  Union\n",
    "\n",
    "import  os, sys, time, shutil\n",
    "from    pathlib             import  Path\n",
    "from    tqdm.notebook       import  tqdm\n",
    "import  pickle\n",
    "import  yaml\n",
    "from    copy                import  deepcopy\n",
    "\n",
    "import  numpy       as  np\n",
    "import  torch\n",
    "from    torch.utils.data            import  TensorDataset, DataLoader\n",
    "\n",
    "path_script = Path(os.getcwd())\n",
    "path_work   = path_script.parent.parent\n",
    "sys.path.append(str(path_work))\n",
    "\n",
    "from    deep_numerical.utils     import  get_time_str\n",
    "from    deep_numerical.utils     import  GaussianNormalizer\n",
    "from    deep_numerical.utils     import  count_parameters, initialize_weights\n",
    "from    deep_numerical.utils     import  relative_error\n",
    "from    deep_numerical.neuralop  import  SFNO\n",
    "\n",
    "from    train_utils     import  load_data, augment_data_2D, exponential_cosine\n",
    "from    train_utils     import  LossFunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Load the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = get_time_str()\n",
    "os.makedirs(f\"./{time_str}\", exist_ok=True)\n",
    "CONFIG_FILE = \"config_train.yaml\"\n",
    "with open(str( path_script / CONFIG_FILE )) as f:\n",
    "    config  = yaml.load(f, Loader = yaml.FullLoader)\n",
    "    _exp    = config['experiment']\n",
    "    _data   = config['pde_dataset']\n",
    "    _model  = config['sfno']\n",
    "DIMENSION = len(_model['n_modes'])\n",
    "\n",
    "# Save the config files\n",
    "shutil.copy(CONFIG_FILE, '/'.join([time_str, CONFIG_FILE]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Set the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Training and data preprocess\n",
    "RANDOM_SEED:    int = _exp['seed']\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "\n",
    "BATCH_SIZE:     int             = _exp['batch_size']\n",
    "NUM_EPOCHS:     int             = _exp['num_epochs']\n",
    "TRAIN_SIZE:     int             = None\n",
    "VAL_SIZE:       int             = None\n",
    "LEARNING_RATE:  float           = _exp['learning_rate']\n",
    "DEVICE:         torch.device    = torch.device(f\"cuda:{_exp['cuda_index']}\")\n",
    "print(f\"* The device to be used >>> {DEVICE}\")\n",
    "\n",
    "RESOLUTION:     int     = _data['resolution']\n",
    "PATH_TRAIN:     Path    = Path(_data['path_train'])\n",
    "PATH_VAL:       Path    = Path(_data['path_val'])\n",
    "\n",
    "NUM_TIME_STEPS: int = _data['num_time_steps']\n",
    "VHS_ALPHA = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Instantiate the storages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_in    = 'data'\n",
    "train_data: dict[str, torch.Tensor] = {\n",
    "    k_in:       None,\n",
    "}\n",
    "val_data: dict[str, torch.Tensor] = {\n",
    "    k_in:       None,\n",
    "}\n",
    "normalizer: dict[str, GaussianNormalizer] = {\n",
    "    k_in:       None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Load the train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "_train_data, _train_info = load_data(PATH_TRAIN, 32, VHS_ALPHA, 1)\n",
    "V_MAX           = _train_info['v_max']\n",
    "WHERE_CLOSED    = _train_info['v_where_closed']\n",
    "_train_data['data'] = _train_data['data'][:, :NUM_TIME_STEPS]\n",
    "_train_data = augment_data_2D(_train_data)\n",
    "\n",
    "train_data[k_in]    = _train_data['data']\n",
    "TRAIN_SIZE  = int(train_data[k_in].shape[0])\n",
    "\n",
    "print(f\"The size of the training dataset >>>\", TRAIN_SIZE, sep=' ')\n",
    "print(f\"The shape of the training dataset >>>\", train_data[k_in].shape, sep=' ')\n",
    "\n",
    "# Validation data\n",
    "_val_data, _ = load_data(PATH_VAL, 32, VHS_ALPHA, 2)\n",
    "_val_data['data'] = _val_data['data'][:, :NUM_TIME_STEPS]\n",
    "val_data[k_in]    = _val_data['data']\n",
    "VAL_SIZE    = int(val_data[k_in].shape[0])\n",
    "print(f\"The size of the validation dataset >>>\", VAL_SIZE, sep=' ')\n",
    "print(f\"The shape of the validation dataset >>>\", val_data[k_in].shape, sep=' ')\n",
    "print('-'*50)\n",
    "print(f\"The number of time steps >>>\", NUM_TIME_STEPS, sep=' ')\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "normalizer[k_in]    = GaussianNormalizer(train_data[k_in])\n",
    "train_data[k_in]    = normalizer[k_in].encode(train_data[k_in])\n",
    "val_data[k_in]      = normalizer[k_in].encode(val_data[k_in])\n",
    "\n",
    "# Save the normalizer, which will also be used in prediction\n",
    "for k in normalizer.keys():\n",
    "    normalizer[k].cpu()\n",
    "torch.save(normalizer, f\"{time_str}/sfno_boltzmann{RESOLUTION}_res{RESOLUTION}_normalizer.pth\")\n",
    "for k in normalizer.keys():\n",
    "    normalizer[k].to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Instantiate dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "train_dataset   = TensorDataset(train_data[k_in])\n",
    "train_loader    = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Validation data\n",
    "val_dataset   = TensorDataset(val_data[k_in])\n",
    "val_loader    = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Instantiate models, loss functions, optimizers, and learning-rate schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SFNO(**_model).to(DEVICE)\n",
    "initialize_weights(model, \"xavier normal\")\n",
    "print(f\"The number of the parameters in the models\\n>>> {count_parameters(model)}\")\n",
    "\n",
    "lf = LossFunctions(dimension=DIMENSION, resolution=RESOLUTION, v_max=V_MAX, v_where_closed=WHERE_CLOSED, device=DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=_exp['learning_rate'])\n",
    "lr_lambda = exponential_cosine(period=20, half_life=100)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history: dict[str, Union[list, float]] = {\n",
    "    'train_rel_error':      [],\n",
    "    'train_rel_error_cons': [],\n",
    "    'val_rel_error':        [],\n",
    "    'val_rel_error_cons':   [],\n",
    "    'train_time':           0.0,\n",
    "}\n",
    "\n",
    "best_val_rel_error:         float   = None\n",
    "best_val_rel_error_cons:    float   = None\n",
    "best_model:         SFNO    = None\n",
    "best_model_idx:     int     = None\n",
    "\n",
    "USED_TIME_STEPS:    int = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_denominator_train  = TRAIN_SIZE * USED_TIME_STEPS * (NUM_TIME_STEPS - USED_TIME_STEPS)\n",
    "_denominator_val    = VAL_SIZE * USED_TIME_STEPS * (NUM_TIME_STEPS - USED_TIME_STEPS)\n",
    "arr_coeff_cons = 0.5 * (1.0-torch.pow(0.9, torch.arange(NUM_EPOCHS, device=DEVICE)))\n",
    "for k in train_data.keys():\n",
    "    normalizer[k].to(DEVICE)\n",
    "\n",
    "elapsed_time = time.time()\n",
    "for epoch in tqdm(range(1, NUM_EPOCHS + 1)):\n",
    "    # NOTE: Train\n",
    "    model.train()\n",
    "    coeff_cons = arr_coeff_cons[epoch-1]\n",
    "    _train_time = time.time()\n",
    "    train_rel_error:        float   = 0\n",
    "    train_rel_error_cons:   float   = 0\n",
    "    for traj, in train_loader:\n",
    "        traj:           torch.Tensor = traj.to(DEVICE)\n",
    "        num_trajectories = len(traj)\n",
    "        \n",
    "        for idx in range(NUM_TIME_STEPS - USED_TIME_STEPS):\n",
    "            # Time-marching (in the scaled space)\n",
    "            data_prev = traj[:, idx]\n",
    "            data_curr = traj[:, idx+1]\n",
    "            data_next = traj[:, idx+2]\n",
    "            pred_curr = model.forward(data_prev)\n",
    "            pred_next = model.forward(pred_curr)\n",
    "            # Descale data\n",
    "            pred_curr = normalizer[k_in].decode(pred_curr)\n",
    "            pred_next = normalizer[k_in].decode(pred_next)\n",
    "            data_curr = normalizer[k_in].decode(data_curr)\n",
    "            data_next = normalizer[k_in].decode(data_next)\n",
    "            # Loss 1 - Data-driven loss\n",
    "            rel_error__data = \\\n",
    "                relative_error(pred_curr, data_curr, p=2).sum() + \\\n",
    "                relative_error(pred_next, data_next, p=2).sum()\n",
    "            # Loss 2 - Conservation loss\n",
    "            rel_error__cons = lf.compute_loss_cons(\n",
    "                [pred_curr, pred_next],\n",
    "                [data_curr, data_next],\n",
    "            )\n",
    "            # Compute the total loss\n",
    "            loss: torch.Tensor = \\\n",
    "                rel_error__data + coeff_cons * rel_error__cons\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Backup\n",
    "            train_rel_error         += rel_error__data.item()\n",
    "            train_rel_error_cons    += rel_error__cons.item()\n",
    "        \n",
    "    scheduler.step()\n",
    "    _train_time = time.time() - _train_time\n",
    "    train_rel_error         /= _denominator_train\n",
    "    train_rel_error_cons    /= _denominator_train\n",
    "    train_history['train_time'] += _train_time\n",
    "    train_history['train_rel_error'].append(train_rel_error)\n",
    "    train_history['train_rel_error_cons'].append(train_rel_error_cons)\n",
    "    \n",
    "    # NOTE: Validation\n",
    "    model.eval()\n",
    "    val_rel_error:      float   = 0\n",
    "    val_rel_error_cons: float   = 0\n",
    "    with torch.no_grad():\n",
    "        for traj, in val_loader:\n",
    "            traj:           torch.Tensor = traj.to(DEVICE)\n",
    "            num_trajectories = len(traj)\n",
    "            \n",
    "            for idx in range(NUM_TIME_STEPS - USED_TIME_STEPS):\n",
    "                # Time-marching (in the scaled space)\n",
    "                data_prev = traj[:, idx]\n",
    "                data_curr = traj[:, idx+1]\n",
    "                data_next = traj[:, idx+2]\n",
    "                pred_curr = model.forward(data_prev)\n",
    "                pred_next = model.forward(pred_curr)\n",
    "                # Descale data\n",
    "                pred_curr = normalizer[k_in].decode(pred_curr)\n",
    "                pred_next = normalizer[k_in].decode(pred_next)\n",
    "                data_curr = normalizer[k_in].decode(data_curr)\n",
    "                data_next = normalizer[k_in].decode(data_next)\n",
    "                # Loss 1 - Data-driven loss\n",
    "                rel_error__data = \\\n",
    "                    relative_error(pred_curr, data_curr, p=2).sum() + \\\n",
    "                    relative_error(pred_next, data_next, p=2).sum()\n",
    "                # Loss 2 - Conservation loss\n",
    "                rel_error__cons = lf.compute_loss_cons(\n",
    "                    [pred_curr, pred_next],\n",
    "                    [data_curr, data_next],\n",
    "                )\n",
    "                # Backup\n",
    "                val_rel_error       += rel_error__data.item()\n",
    "                val_rel_error_cons  += rel_error__cons.item()\n",
    "            \n",
    "    val_rel_error       /= _denominator_val\n",
    "    val_rel_error_cons  /= _denominator_val\n",
    "    train_history['val_rel_error'].append(val_rel_error)\n",
    "    train_history['val_rel_error_cons'].append(val_rel_error_cons)\n",
    "    \n",
    "    # NOTE: Save the best model\n",
    "    if (\n",
    "            (best_val_rel_error is None or best_val_rel_error_cons is None) or \n",
    "            (best_val_rel_error+best_val_rel_error_cons > val_rel_error+val_rel_error_cons)\n",
    "        ):\n",
    "        best_val_rel_error      = val_rel_error\n",
    "        best_val_rel_error_cons = val_rel_error_cons\n",
    "        best_model      = deepcopy(model)\n",
    "        best_model_idx  = epoch-1\n",
    "        \n",
    "    # NOTE: Report\n",
    "    print(f\"[ Epoch {epoch} / {NUM_EPOCHS} | coeff_cons: {coeff_cons:.4e} , lr: {scheduler.get_last_lr()[0]:.4e}]\")\n",
    "    print(\n",
    "        \"# Best model:\",\n",
    "        f\"epoch {best_model_idx+1}\",\n",
    "        f\"({train_history['val_rel_error'][best_model_idx]:.4e}\",\n",
    "        f\"|\",\n",
    "        f\"{train_history['val_rel_error_cons'][best_model_idx]:.4e})\",\n",
    "        sep=' ',\n",
    "    )\n",
    "    print(\n",
    "        \"* [train] Relative error:\",\n",
    "        f\"{train_history['train_rel_error'][-1]:.4e}\",\n",
    "        f\"|\",\n",
    "        f\"{train_history['train_rel_error_cons'][-1]:.4e}\",\n",
    "        sep=' ',\n",
    "    )\n",
    "    print(\n",
    "        \"* [valid] Relative error:\",\n",
    "        f\"{train_history['val_rel_error'][-1]:.4e}\",\n",
    "        f\"|\",\n",
    "        f\"{train_history['val_rel_error_cons'][-1]:.4e}\",\n",
    "        sep=' ',\n",
    "    )\n",
    "    print('\\n')\n",
    "        \n",
    "    if (\n",
    "            best_val_rel_error < 1e-5 and\n",
    "            train_history['val_rel_error_cons'][best_model_idx] < 1e-5\n",
    "        ):\n",
    "        print(f\"Early stopping at epoch {epoch}.\")\n",
    "        break\n",
    "    \n",
    "    if epoch%10==0:\n",
    "        # Save the model\n",
    "        __dir_checkpoint = Path(f\"./checkpoint__{time_str}\")\n",
    "        __name_checkpoint = f\"sfno_boltzmann{RESOLUTION}_res{RESOLUTION}_epoch{str(epoch).zfill(len(str(NUM_EPOCHS)))}.pth\"\n",
    "        os.makedirs(__dir_checkpoint, exist_ok=True)\n",
    "        model.cpu()\n",
    "        best_model.cpu()\n",
    "        torch.save(model.state_dict(), __dir_checkpoint/__name_checkpoint)\n",
    "        torch.save(model.state_dict(), __dir_checkpoint/(\"best__\" + __name_checkpoint))\n",
    "        model.to(DEVICE)\n",
    "        best_model.to(DEVICE)\n",
    "        \n",
    "elapsed_time = time.time() - elapsed_time\n",
    "print(f\"Elapsed time: {int(elapsed_time)} seconds\")\n",
    "\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Save the model and the train history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "for k in train_data.keys():\n",
    "    normalizer[k].cpu()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), f\"{time_str}/sfno_boltzmann{RESOLUTION}_res{RESOLUTION}.pth\")\n",
    "\n",
    "# Save the history\n",
    "with open(f\"{time_str}/sfno_boltzmann{RESOLUTION}_res{RESOLUTION}.pickle\", \"wb\") as f:\n",
    "    pickle.dump(train_history, f)\n",
    "\n",
    "# Clear the GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Done\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GJ2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
