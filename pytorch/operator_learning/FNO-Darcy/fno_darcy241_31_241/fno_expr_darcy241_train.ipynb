{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m path_root \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/media/junseung/47a90e46-3a9d-467c-bbee-066752b68532/GWANGJAE/python_deep_numerical/pytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(path_root))\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m    \u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneuralop\u001b[39;00m    \u001b[38;5;28;01mimport\u001b[39;00m  FourierNeuralOperator   \u001b[38;5;28;01mas\u001b[39;00m  FNO\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch'"
     ]
    }
   ],
   "source": [
    "from    typing  import  Callable\n",
    "\n",
    "import  os, time, sys\n",
    "from    pathlib             import  Path\n",
    "from    tqdm.notebook       import  tqdm\n",
    "import  pickle\n",
    "import  yaml\n",
    "\n",
    "import  numpy       as  np\n",
    "import  torch\n",
    "from    torch.utils.data            import  TensorDataset, DataLoader\n",
    "\n",
    "path_root = Path(r\"/media/junseung/47a90e46-3a9d-467c-bbee-066752b68532/GWANGJAE/python_deep_numerical/pytorch\")\n",
    "sys.path.append(str(path_root))\n",
    "from    neuralop    import  FourierNeuralOperator   as  FNO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Load the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_train.yaml\") as f:\n",
    "    config = yaml.load(f, Loader = yaml.FullLoader)\n",
    "    _exp   = config['experiment']\n",
    "    _data  = config['pde_dataset']\n",
    "    _fno   = config['fno']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Set the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Training and data preprocess\n",
    "\n",
    "\n",
    "BATCH_SIZE      = _exp['batch_size']\n",
    "NUM_EPOCHS      = _exp['num_epochs']\n",
    "LEARNING_RATE   = _exp['learning_rate']\n",
    "TRAIN_SIZE      = _exp['train_size']\n",
    "VAL_SIZE        = _exp['val_size']\n",
    "DEVICE          = torch.device(f\"cuda:{_exp['cuda_index']}\")\n",
    "\n",
    "\n",
    "RESOLUTION      = _data['resolution']\n",
    "TRAIN_PATH      = Path(_data['path'])\n",
    "__RANDOM_CHOICE = np.random.choice(1024, TRAIN_SIZE + VAL_SIZE, replace = False)\n",
    "TRAIN_MASK      = __RANDOM_CHOICE[:TRAIN_SIZE]\n",
    "VAL_MASK        = __RANDOM_CHOICE[-VAL_SIZE:]\n",
    "\n",
    "\n",
    "DOWNSAMPLE      = _data['downsample']\n",
    "GRID            = (RESOLUTION - 1) // DOWNSAMPLE + 1\n",
    "grid            = torch.stack(\n",
    "                        torch.meshgrid(\n",
    "                            torch.linspace(0, 1, GRID),\n",
    "                            torch.linspace(0, 1, GRID),\n",
    "                            indexing = 'ij'\n",
    "                        ),\n",
    "                        dim = -1\n",
    "                    )   # Shape: (GRID, GRID, dim_domain = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Instantiate the storages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data: dict[str, torch.Tensor] = {\n",
    "    'grid':     None,\n",
    "    'coeff':    None,\n",
    "    'sol':      None,\n",
    "}\n",
    "val_data: dict[str, torch.Tensor] = {\n",
    "    'grid':     None,\n",
    "    'coeff':    None,\n",
    "    'sol':      None,\n",
    "}\n",
    "\n",
    "\n",
    "normalizer: dict[str, GaussianNormalizer] = {\n",
    "    'grid':     None,\n",
    "    'coeff':    None,\n",
    "    'sol':      None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Load the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc3c8d064d8407da6d91eab0d17fb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing the train data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd46c0fa7a2c40cab58abbe27f50edcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing the validation data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train data\n",
    "reader = npzReader(TRAIN_PATH)\n",
    "for cnt, k in tqdm(enumerate(train_data.keys()), desc = \"Preprocessing the train data\"):   \n",
    "    # Step 1. Load data\n",
    "    if k != 'grid':\n",
    "        train_data[k] = torch.from_numpy(reader.get_field(k)[TRAIN_MASK, ::DOWNSAMPLE, ::DOWNSAMPLE])\n",
    "        train_data[k] = train_data[k].type(torch.float).to(DEVICE)\n",
    "    else:\n",
    "        train_data[k] = grid.clone().unsqueeze(0).repeat(TRAIN_SIZE, 1, 1, 1).to(DEVICE)\n",
    "    \n",
    "    # Step 2. Normalize data\n",
    "    normalizer[k] = GaussianNormalizer(train_data[k])\n",
    "    normalizer[k].to(DEVICE)\n",
    "    train_data[k] = normalizer[k].encode(train_data[k])\n",
    "\n",
    "\n",
    "# Validation data\n",
    "for cnt, k in tqdm(enumerate(val_data.keys()), desc = \"Preprocessing the validation data\"):\n",
    "    # Step 1. Load data\n",
    "    if k != 'grid':\n",
    "        val_data[k] = torch.from_numpy(reader.get_field(k)[VAL_MASK, ::DOWNSAMPLE, ::DOWNSAMPLE])\n",
    "        val_data[k] = val_data[k].type(torch.float).to(DEVICE)\n",
    "    else:\n",
    "        val_data[k] = grid.clone().unsqueeze(0).repeat(VAL_SIZE, 1, 1, 1).to(DEVICE)\n",
    "    \n",
    "    # Step 2. Normalize data (NOTE: Uses the normalizers for the train dataset)\n",
    "    val_data[k] = normalizer[k].encode(val_data[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Merge the grid and the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['data'] = torch.cat([train_data['grid'], train_data['coeff']], dim = -1)\n",
    "val_data['data']   = torch.cat([  val_data['grid'], val_data['coeff']]  , dim = -1)\n",
    "del(train_data['grid'], train_data['coeff'])\n",
    "del(  val_data['grid'], val_data['coeff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. Instantiate dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_data['data'], train_data['sol'])\n",
    "val_dataset   = TensorDataset(  val_data['data'], val_data['sol'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader   = torch.utils.data.DataLoader(  val_dataset,  batch_size = BATCH_SIZE, shuffle = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Initialize the model and instantiate the loss function and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of the parameters in the custom FNO\n",
      ">>> 696865\n",
      "FourierNeuralOperator(\n",
      "    lift:       MLP(layer=(3, 64, 32), bias=True, activation=relu)\n",
      "    hidden:     (\n",
      "                     FourierLayer(n_modes=(12, 12)),\n",
      "                     ReLU(),\n",
      "                     FourierLayer(n_modes=(12, 12)),\n",
      "                     ReLU(),\n",
      "                     FourierLayer(n_modes=(12, 12)),\n",
      "                     ReLU(),\n",
      "                     FourierLayer(n_modes=(12, 12)),\n",
      "                ),\n",
      "    projection: MLP(layer=(32, 64, 1), bias=True, activation=relu)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Programming\\Operator learning\\FNO-Darcy\\fno_darcy241_31_241\\custom_modules\\core\\_helper.py:52: UserWarning: (FourierNeuralOperator) The following unnecessary arguments are given to the initializer.\n",
      "* kwargs[0] >>> activation=relu\n",
      "Note that these arguments are ignored.\n",
      "  warnings.warn('\\n'.join(msg), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "fno = FourierNeuralOperator(**_fno).to(DEVICE)\n",
    "print(f\"The number of the parameters in the custom FNO\\n>>> {count_parameters(fno)}\")\n",
    "print(fno)\n",
    "\n",
    "for p in fno.parameters():\n",
    "    if p.ndim == 1:\n",
    "        torch.nn.init.zeros_(p)\n",
    "    else:\n",
    "        torch.nn.init.xavier_uniform_(p)\n",
    "\n",
    "# criterion = torch.nn.MSELoss(reduction = 'mean')\n",
    "criterion: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = \\\n",
    "    lambda pred, target: \\\n",
    "        torch.sqrt(\n",
    "            (pred-target).pow(2).mean(dim=range(1,pred.ndim)) / \\\n",
    "            target.pow(2).mean(dim=range(1,target.ndim))\n",
    "        )\n",
    "optimizer = torch.optim.Adam(params = fno.parameters(), lr = _exp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 7, 32, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fno.network_hidden[0].spectral.kernel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb3918ce5f544438172e5e5914b6ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 1 / 300 ]\n",
      "* train_loss     : 2.6722e-01\n",
      "* train_error    : 2.5995e-01\n",
      "* val_loss       : 3.0806e-02\n",
      "* val_error      : 1.0337e-01\n",
      "[ Epoch 10 / 300 ]\n",
      "* train_loss     : 9.7002e-04\n",
      "* train_error    : 1.8442e-02\n",
      "* val_loss       : 1.2733e-03\n",
      "* val_error      : 2.0665e-02\n",
      "[ Epoch 20 / 300 ]\n",
      "* train_loss     : 4.9055e-04\n",
      "* train_error    : 1.3117e-02\n",
      "* val_loss       : 9.2122e-04\n",
      "* val_error      : 1.7790e-02\n",
      "[ Epoch 30 / 300 ]\n",
      "* train_loss     : 3.6295e-04\n",
      "* train_error    : 1.1308e-02\n",
      "* val_loss       : 7.0815e-04\n",
      "* val_error      : 1.5516e-02\n",
      "[ Epoch 40 / 300 ]\n",
      "* train_loss     : 4.3349e-04\n",
      "* train_error    : 1.2264e-02\n",
      "* val_loss       : 8.3069e-04\n",
      "* val_error      : 1.6963e-02\n",
      "[ Epoch 50 / 300 ]\n",
      "* train_loss     : 3.9345e-04\n",
      "* train_error    : 1.1585e-02\n",
      "* val_loss       : 6.3545e-04\n",
      "* val_error      : 1.4553e-02\n",
      "[ Epoch 60 / 300 ]\n",
      "* train_loss     : 3.1376e-04\n",
      "* train_error    : 1.0497e-02\n",
      "* val_loss       : 6.8144e-04\n",
      "* val_error      : 1.5335e-02\n",
      "[ Epoch 70 / 300 ]\n",
      "* train_loss     : 4.7267e-04\n",
      "* train_error    : 1.2753e-02\n",
      "* val_loss       : 8.2794e-04\n",
      "* val_error      : 1.7023e-02\n",
      "[ Epoch 80 / 300 ]\n",
      "* train_loss     : 1.9423e-04\n",
      "* train_error    : 8.2684e-03\n",
      "* val_loss       : 5.8534e-04\n",
      "* val_error      : 1.4117e-02\n",
      "[ Epoch 90 / 300 ]\n",
      "* train_loss     : 3.6956e-04\n",
      "* train_error    : 1.1279e-02\n",
      "* val_loss       : 7.5044e-04\n",
      "* val_error      : 1.6145e-02\n",
      "[ Epoch 100 / 300 ]\n",
      "* train_loss     : 3.0025e-04\n",
      "* train_error    : 1.0153e-02\n",
      "* val_loss       : 7.6530e-04\n",
      "* val_error      : 1.6210e-02\n",
      "[ Epoch 110 / 300 ]\n",
      "* train_loss     : 1.8364e-04\n",
      "* train_error    : 8.0140e-03\n",
      "* val_loss       : 5.8626e-04\n",
      "* val_error      : 1.4135e-02\n",
      "[ Epoch 120 / 300 ]\n",
      "* train_loss     : 1.9970e-04\n",
      "* train_error    : 8.3489e-03\n",
      "* val_loss       : 5.8338e-04\n",
      "* val_error      : 1.4203e-02\n",
      "[ Epoch 130 / 300 ]\n",
      "* train_loss     : 1.3823e-04\n",
      "* train_error    : 6.9805e-03\n",
      "* val_loss       : 5.6450e-04\n",
      "* val_error      : 1.3923e-02\n",
      "[ Epoch 140 / 300 ]\n",
      "* train_loss     : 2.2126e-04\n",
      "* train_error    : 8.6812e-03\n",
      "* val_loss       : 6.6196e-04\n",
      "* val_error      : 1.4985e-02\n",
      "[ Epoch 150 / 300 ]\n",
      "* train_loss     : 2.7331e-04\n",
      "* train_error    : 9.7293e-03\n",
      "* val_loss       : 6.3249e-04\n",
      "* val_error      : 1.4771e-02\n",
      "[ Epoch 160 / 300 ]\n",
      "* train_loss     : 1.5516e-04\n",
      "* train_error    : 7.3851e-03\n",
      "* val_loss       : 5.8065e-04\n",
      "* val_error      : 1.4104e-02\n",
      "[ Epoch 170 / 300 ]\n",
      "* train_loss     : 2.2522e-04\n",
      "* train_error    : 8.7783e-03\n",
      "* val_loss       : 5.5475e-04\n",
      "* val_error      : 1.3879e-02\n",
      "[ Epoch 180 / 300 ]\n",
      "* train_loss     : 2.5117e-04\n",
      "* train_error    : 9.3077e-03\n",
      "* val_loss       : 5.7027e-04\n",
      "* val_error      : 1.3960e-02\n",
      "[ Epoch 190 / 300 ]\n",
      "* train_loss     : 1.2585e-04\n",
      "* train_error    : 6.6062e-03\n",
      "* val_loss       : 6.1291e-04\n",
      "* val_error      : 1.4539e-02\n",
      "[ Epoch 200 / 300 ]\n",
      "* train_loss     : 1.5431e-04\n",
      "* train_error    : 7.3324e-03\n",
      "* val_loss       : 5.5062e-04\n",
      "* val_error      : 1.3799e-02\n",
      "[ Epoch 210 / 300 ]\n",
      "* train_loss     : 3.8623e-04\n",
      "* train_error    : 1.1530e-02\n",
      "* val_loss       : 7.1431e-04\n",
      "* val_error      : 1.5856e-02\n",
      "[ Epoch 220 / 300 ]\n",
      "* train_loss     : 7.8415e-05\n",
      "* train_error    : 5.2468e-03\n",
      "* val_loss       : 4.9289e-04\n",
      "* val_error      : 1.2870e-02\n",
      "[ Epoch 230 / 300 ]\n",
      "* train_loss     : 2.5490e-04\n",
      "* train_error    : 9.3438e-03\n",
      "* val_loss       : 6.0514e-04\n",
      "* val_error      : 1.4516e-02\n",
      "[ Epoch 240 / 300 ]\n",
      "* train_loss     : 7.5089e-05\n",
      "* train_error    : 5.1261e-03\n",
      "* val_loss       : 5.5352e-04\n",
      "* val_error      : 1.3502e-02\n",
      "[ Epoch 250 / 300 ]\n",
      "* train_loss     : 2.5034e-04\n",
      "* train_error    : 9.2496e-03\n",
      "* val_loss       : 7.3910e-04\n",
      "* val_error      : 1.5704e-02\n",
      "[ Epoch 260 / 300 ]\n",
      "* train_loss     : 9.9352e-05\n",
      "* train_error    : 5.8437e-03\n",
      "* val_loss       : 4.9813e-04\n",
      "* val_error      : 1.2836e-02\n",
      "[ Epoch 270 / 300 ]\n",
      "* train_loss     : 1.2844e-04\n",
      "* train_error    : 6.6731e-03\n",
      "* val_loss       : 5.2054e-04\n",
      "* val_error      : 1.3276e-02\n",
      "[ Epoch 280 / 300 ]\n",
      "* train_loss     : 1.0122e-04\n",
      "* train_error    : 5.9162e-03\n",
      "* val_loss       : 4.9672e-04\n",
      "* val_error      : 1.3027e-02\n",
      "[ Epoch 290 / 300 ]\n",
      "* train_loss     : 9.3798e-05\n",
      "* train_error    : 5.7126e-03\n",
      "* val_loss       : 5.6323e-04\n",
      "* val_error      : 1.3764e-02\n",
      "[ Epoch 300 / 300 ]\n",
      "* train_loss     : 1.4401e-04\n",
      "* train_error    : 6.9596e-03\n",
      "* val_loss       : 4.8868e-04\n",
      "* val_error      : 1.2970e-02\n",
      "Elapsed time: 137 seconds\n"
     ]
    }
   ],
   "source": [
    "train_history = {\n",
    "    'train_loss':   [],\n",
    "    'train_error':  [],\n",
    "    'val_loss':     [],\n",
    "    'val_error':    [],\n",
    "    'train_time':   0.0,\n",
    "}\n",
    "normalizer['sol'].to(DEVICE)\n",
    "\n",
    "elapsed_time = time.time()\n",
    "for epoch in tqdm(range(1, NUM_EPOCHS + 1)):\n",
    "    # NOTE: Train\n",
    "    fno.train()\n",
    "    _train_time = time.time()\n",
    "    train_epoch_loss:  torch.Tensor = 0\n",
    "    train_epoch_error: torch.Tensor = 0\n",
    "    for data, target in train_loader:\n",
    "        num_data = len(data)\n",
    "        \n",
    "        train_pred = fno.forward(data)\n",
    "        train_loss = criterion.forward(train_pred, target)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss = train_epoch_loss + train_loss * num_data\n",
    "        train_pred  = normalizer['sol'].decode(train_pred)\n",
    "        target      = normalizer['sol'].decode(target)\n",
    "        train_epoch_error = train_epoch_error + (\n",
    "            torch.linalg.norm(train_pred - target) / (1e-8 + torch.linalg.norm(target))\n",
    "        ) * num_data\n",
    "    _train_time = time.time() - _train_time\n",
    "    train_history['train_time'] += _train_time\n",
    "    train_epoch_loss    = train_epoch_loss / TRAIN_SIZE\n",
    "    train_epoch_error   = train_epoch_error / TRAIN_SIZE\n",
    "    train_history['train_loss'].append(train_epoch_loss.item())\n",
    "    train_history['train_error'].append(train_epoch_error.item())\n",
    "    \n",
    "    \n",
    "    # NOTE: Validation\n",
    "    fno.eval()\n",
    "    val_epoch_loss:     torch.Tensor = 0\n",
    "    val_epoch_error:    torch.Tensor = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            num_data = len(data)\n",
    "            \n",
    "            val_pred = fno.forward(data)\n",
    "            val_loss = criterion.forward(val_pred, target)\n",
    "            \n",
    "            val_epoch_loss      = val_epoch_loss + val_loss * num_data\n",
    "            val_pred = normalizer['sol'].decode(val_pred)\n",
    "            target   = normalizer['sol'].decode(target)\n",
    "            val_epoch_error     = val_epoch_error + (\n",
    "                                        torch.linalg.norm(val_pred - target) / (1e-8 + torch.linalg.norm(target))\n",
    "                                    ) * num_data\n",
    "    val_epoch_loss      = val_epoch_loss / VAL_SIZE\n",
    "    val_epoch_error     = val_epoch_error / VAL_SIZE\n",
    "    train_history['val_loss'].append(val_epoch_loss.item())\n",
    "    train_history['val_error'].append(val_epoch_error.item())\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"[ Epoch {epoch} / {NUM_EPOCHS} ]\")\n",
    "        for k in train_history.keys():\n",
    "            if k == \"train_time\":\n",
    "                continue\n",
    "            print(f\"* {k:15s}: {train_history[k][-1]:.4e}\")\n",
    "    \n",
    "elapsed_time = time.time() - elapsed_time\n",
    "print(f\"Elapsed time: {int(elapsed_time)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Save the model and the train history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "os.makedirs(f\"./{time_str}\", exist_ok = True)\n",
    "torch.save(fno.state_dict(), f\"{time_str}/fno_darcy{RESOLUTION}_res{GRID}.pth\")\n",
    "\n",
    "# Save the normalizer, which will also be used in prediction\n",
    "normalizer['sol'].cpu()\n",
    "torch.save(normalizer, f\"{time_str}/fno_darcy{RESOLUTION}_res{GRID}_normalizer.pth\")\n",
    "\n",
    "# Save the history\n",
    "with open(f\"{time_str}/fno_darcy{RESOLUTION}_res{GRID}.pickle\", \"wb\") as f:\n",
    "    pickle.dump(train_history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GJ2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
