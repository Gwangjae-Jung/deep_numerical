{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os, time\n",
    "from    pathlib             import  Path\n",
    "from    tqdm.notebook       import  tqdm\n",
    "import  pickle\n",
    "import  yaml\n",
    "\n",
    "import  numpy       as  np\n",
    "import  torch\n",
    "from    torch       import  nn\n",
    "from    torch.utils.data            import  TensorDataset, DataLoader\n",
    "\n",
    "from    custom_modules.utils                import  get_time_str\n",
    "from    custom_modules.utils                import  GridGenerator, npzReader, GaussianNormalizer\n",
    "from    custom_modules.pytorch.neuralop     import  DeepONetUnstructured  as  DeepONet\n",
    "from    custom_modules.pytorch.torch_utils  import  count_parameters\n",
    "from    custom_modules.pytorch.gradients    import  *\n",
    "\n",
    "\n",
    "time_str = get_time_str()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Load the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = get_time_str()\n",
    "with open(\"config_train.yaml\") as f:\n",
    "    config      = yaml.load(f, Loader = yaml.FullLoader)\n",
    "    _exp        = config['experiment']\n",
    "    _data       = config['pde_dataset']\n",
    "    _deeponet   = config['deeponet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Set the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Training and data preprocess\n",
    "\n",
    "\n",
    "BATCH_SIZE      = _exp['batch_size']\n",
    "NUM_EPOCHS      = _exp['num_epochs']\n",
    "LEARNING_RATE   = _exp['learning_rate']\n",
    "TRAIN_SIZE      = _exp['train_size']\n",
    "VAL_SIZE        = _exp['val_size']\n",
    "DEVICE          = torch.device(f\"cuda:{_exp['cuda_index']}\")\n",
    "\n",
    "\n",
    "RESOLUTION      = _data['resolution']\n",
    "TRAIN_PATH      = Path(_data['path'])\n",
    "__RANDOM_CHOICE = np.random.choice(1024, TRAIN_SIZE + VAL_SIZE, replace = False)\n",
    "TRAIN_MASK      = __RANDOM_CHOICE[:TRAIN_SIZE]\n",
    "VAL_MASK        = __RANDOM_CHOICE[-VAL_SIZE:]\n",
    "\n",
    "\n",
    "DOWNSAMPLE      = _data['downsample']\n",
    "GRID            = (RESOLUTION - 1) // DOWNSAMPLE + 1\n",
    "grid            = torch.stack(\n",
    "                        torch.meshgrid(\n",
    "                            torch.linspace(0, 1, GRID),\n",
    "                            torch.linspace(0, 1, GRID),\n",
    "                            indexing = 'ij',\n",
    "                        ),\n",
    "                        dim = -1\n",
    "                    ).requires_grad_()   # Shape: (GRID, GRID, dim_domain = 2)\n",
    "grid            = grid.reshape(-1, 2).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Instantiate the storages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data: dict[str, torch.Tensor] = {\n",
    "    'coeff':        None,\n",
    "    'Kcoeff':       None,\n",
    "    'Kcoeff_x':     None,\n",
    "    'Kcoeff_y':     None,\n",
    "    'sol':          None,\n",
    "}\n",
    "val_data: dict[str, torch.Tensor] = {\n",
    "    'coeff':        None,\n",
    "    'Kcoeff':       None,\n",
    "    'Kcoeff_x':     None,\n",
    "    'Kcoeff_y':     None,\n",
    "    'sol':          None,\n",
    "}\n",
    "\n",
    "\n",
    "normalizer: dict[str, GaussianNormalizer] = {\n",
    "    'coeff':    None,\n",
    "    'Kcoeff':   None,\n",
    "    # FOR THE DERIVATIVES, THE NORMALIZER FOR THE MOLLIFIED COEFFICIENT FUNCTIONS IS USED\n",
    "    'sol':      None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Load the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34957559c9c044ff8f86d40d47268d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing the train data:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ab18b1e4104482bb1cf6b3634c218f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing the validation data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['coeff', 'Kcoeff', 'sol', 'Kcoeff_grad', 'grid'])\n",
      "dict_keys(['coeff', 'Kcoeff', 'sol', 'Kcoeff_grad', 'grid'])\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "reader = npzReader(TRAIN_PATH)\n",
    "for k in tqdm(train_data.keys(), desc = \"Preprocessing the train data\"):   \n",
    "    # Step 1. Load data\n",
    "    train_data[k] = torch.from_numpy(reader.get_field(k)[TRAIN_MASK, ::DOWNSAMPLE, ::DOWNSAMPLE])\n",
    "    train_data[k] = train_data[k].type(torch.float)\n",
    "    train_data[k] = train_data[k].reshape(TRAIN_SIZE, -1).to(DEVICE)\n",
    "    \n",
    "    # Step 2. Normalize data\n",
    "    normalizer[k] = GaussianNormalizer(train_data[k])\n",
    "    normalizer[k].to(DEVICE)\n",
    "    if k not in ('Kcoeff_x', 'Kcoeff_y'):\n",
    "        train_data[k] = normalizer[k].encode(train_data[k])\n",
    "\n",
    "\n",
    "train_data['Kcoeff_grad'] = torch.stack([train_data['Kcoeff_x'], train_data['Kcoeff_y']], dim = -1)\n",
    "train_data['Kcoeff_grad'] = normalizer['coeff'].encode(train_data['Kcoeff_grad'])\n",
    "train_data.pop('Kcoeff_x')\n",
    "train_data.pop('Kcoeff_y')\n",
    "train_data['grid'] = grid.unsqueeze(0).repeat(len(train_data['coeff']), 1, 1)\n",
    "\n",
    "\n",
    "# Validation data\n",
    "for cnt, k in tqdm(enumerate(val_data.keys()), desc = \"Preprocessing the validation data\"):\n",
    "    # Step 1. Load data\n",
    "    val_data[k] = torch.from_numpy(reader.get_field(k)[VAL_MASK, ::DOWNSAMPLE, ::DOWNSAMPLE])\n",
    "    val_data[k] = val_data[k].type(torch.float)\n",
    "    val_data[k] = val_data[k].reshape(VAL_SIZE, -1).to(DEVICE)\n",
    "    \n",
    "    # Step 2. Normalize data (NOTE: Uses the normalizers for the train dataset)\n",
    "    if k not in ('Kcoeff_x', 'Kcoeff_y'):\n",
    "        val_data[k] = normalizer[k].encode(val_data[k])\n",
    "\n",
    "\n",
    "val_data['Kcoeff_grad'] = torch.stack([val_data['Kcoeff_x'], val_data['Kcoeff_y']], dim = -1)\n",
    "val_data['Kcoeff_grad'] = normalizer['coeff'].encode(val_data['Kcoeff_grad'])\n",
    "val_data.pop('Kcoeff_x')\n",
    "val_data.pop('Kcoeff_y')\n",
    "val_data['grid'] = grid.unsqueeze(0).repeat(len(val_data['coeff']), 1, 1)\n",
    "\n",
    "print(train_data.keys())\n",
    "print(val_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Instantiate dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(\n",
    "                    train_data['grid'], train_data['coeff'],\n",
    "                    train_data['Kcoeff'], train_data['Kcoeff_grad'],\n",
    "                    train_data['sol']\n",
    "                )\n",
    "val_dataset   = TensorDataset(\n",
    "                    val_data['grid'], val_data['coeff'],\n",
    "                    val_data['Kcoeff'], val_data['Kcoeff_grad'],\n",
    "                    val_data['sol']\n",
    "                )\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader   = torch.utils.data.DataLoader(  val_dataset,  batch_size = BATCH_SIZE, shuffle = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Initialize the model and instantiate the loss function and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of the parameters in the model\n",
      ">>> 1018881\n",
      "DeepONet(\n",
      "    unstructured,\n",
      "    branch=MLP(layer=(961, 512, 512, 256), bias=True, activation=relu),\n",
      "    trunk =MLP(layer=(2, 256, 256, 256), bias=True, activation=relu),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "deeponet = DeepONet(**_deeponet).to(DEVICE)\n",
    "print(f\"The number of the parameters in the model\\n>>> {count_parameters(deeponet)}\")\n",
    "print(deeponet)\n",
    "\n",
    "for p in deeponet.parameters():\n",
    "    if p.ndim == 1:\n",
    "        torch.nn.init.zeros_(p)\n",
    "    else:\n",
    "        torch.nn.init.xavier_uniform_(p)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction = 'mean')\n",
    "optimizer = torch.optim.Adam(params = deeponet.parameters(), lr = _exp['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_gov(\n",
    "        grid:       torch.Tensor,\n",
    "        pred:       torch.Tensor,\n",
    "        data:       torch.Tensor,\n",
    "        data_grad:  torch.Tensor,\n",
    "        verbose:    bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Note that the forcing term is set to be the unity.\n",
    "    \n",
    "    * `grid` is of shape `(B, num_nodes, dim_space)`\n",
    "    * `pred` is of shape `(B, num_nodes, channel=1)`\n",
    "    * `data` is of shape `(B, num_nodes, channel=1)`\n",
    "    * `data_grad` is of shape `(B, num_nodes, dim_space)`\n",
    "    \"\"\"\n",
    "    forcing_term    = 1\n",
    "    pred_grad       = compute_grad(pred, grid)\n",
    "    pred_laplacian  = torch.stack(\n",
    "                            [\n",
    "                                compute_grad(pred_grad[..., d], grid)[..., d]\n",
    "                                for d in range(grid.shape[-1])\n",
    "                            ], dim = -1\n",
    "                        ).sum( dim = -1 )\n",
    "    if verbose:\n",
    "        print(f\"grid.shape\\n>>> {grid.shape}\")\n",
    "        print(f\"pred.shape\\n>>> {pred.shape}\")\n",
    "        print(f\"pred_grad.shape\\n>>> {pred_grad.shape}\")\n",
    "        print(f\"data.shape\\n>>> {data.shape}\")\n",
    "        print(f\"data_grad.shape\\n>>> {data_grad.shape}\")\n",
    "        print(f\"pred_laplacian.shape\\n>>> {pred_laplacian.shape}\")\n",
    "    \n",
    "    assert data_grad.shape == pred_grad.shape, \\\n",
    "        f\"{data_grad.shape} and {pred_grad.shape}\"\n",
    "    v1 = (data_grad * pred_grad).sum(dim = -1)\n",
    "    \n",
    "    assert data.shape == pred_laplacian.shape, \\\n",
    "        f\"{data.shape} and {pred_laplacian.shape}\"\n",
    "    v2 = data * pred_laplacian\n",
    "    \n",
    "    assert v1.shape == v2.shape\n",
    "    lhs = -(v1 + v2)\n",
    "    return torch.pow(lhs - forcing_term, 2).mean()\n",
    "    \n",
    "\n",
    "def compute_loss_bc(\n",
    "        pred:       torch.Tensor,\n",
    "        target:     torch.Tensor,\n",
    "        boundary:   torch.Tensor | slice,\n",
    "    ) -> torch.Tensor:\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 2.6000, 3.2000, 3.8000, 4.4000, 5.0000, 5.6000, 6.2000, 6.8000,\n",
      "         7.4000, 8.0000],\n",
      "        [2.0000, 2.6000, 3.2000, 3.8000, 4.4000, 5.0000, 5.6000, 6.2000, 6.8000,\n",
      "         7.4000, 8.0000],\n",
      "        [2.0000, 2.6000, 3.2000, 3.8000, 4.4000, 5.0000, 5.6000, 6.2000, 6.8000,\n",
      "         7.4000, 8.0000],\n",
      "        [2.0000, 2.6000, 3.2000, 3.8000, 4.4000, 5.0000, 5.6000, 6.2000, 6.8000,\n",
      "         7.4000, 8.0000],\n",
      "        [2.0000, 2.6000, 3.2000, 3.8000, 4.4000, 5.0000, 5.6000, 6.2000, 6.8000,\n",
      "         7.4000, 8.0000],\n",
      "        [2.0000, 2.6000, 3.2000, 3.8000, 4.4000, 5.0000, 5.6000, 6.2000, 6.8000,\n",
      "         7.4000, 8.0000],\n",
      "        [2.0000, 2.6000, 3.2000, 3.8000, 4.4000, 5.0000, 5.6000, 6.2000, 6.8000,\n",
      "         7.4000, 8.0000],\n",
      "        [2.0000, 2.6000, 3.2000, 3.8000, 4.4000, 5.0000, 5.6000, 6.2000, 6.8000,\n",
      "         7.4000, 8.0000],\n",
      "        [2.0000, 2.6000, 3.2000, 3.8000, 4.4000, 5.0000, 5.6000, 6.2000, 6.8000,\n",
      "         7.4000, 8.0000],\n",
      "        [2.0000, 2.6000, 3.2000, 3.8000, 4.4000, 5.0000, 5.6000, 6.2000, 6.8000,\n",
      "         7.4000, 8.0000],\n",
      "        [2.0000, 2.6000, 3.2000, 3.8000, 4.4000, 5.0000, 5.6000, 6.2000, 6.8000,\n",
      "         7.4000, 8.0000]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "mygrid = torch.meshgrid(torch.linspace(0, 1, 11), torch.linspace(0, 1, 11), indexing = 'ij')\n",
    "mygrid = torch.stack(mygrid, dim = -1)\n",
    "mygrid.requires_grad_()\n",
    "\n",
    "\n",
    "u = mygrid[..., 0] ** 2 + mygrid[..., 1] ** 3\n",
    "d1u = compute_grad(u, mygrid)\n",
    "d2u = torch.stack([compute_grad(d1u[..., d], mygrid)[..., d] for d in range(mygrid.size(-1))], dim = -1)\n",
    "lapu = d2u.sum(-1)\n",
    "print(lapu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f587efe858a94604bcebd2aa80871c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 1 / 300 ]\n",
      "* train_loss     : 1.6734e+01\n",
      "* train_error    : 6.4041e-01\n",
      "* val_loss       : 9.6595e-01\n",
      "* val_error      : 5.9124e-01\n",
      "\n",
      "[ Epoch 10 / 300 ]\n",
      "* train_loss     : 1.6393e+00\n",
      "* train_error    : 5.8958e-01\n",
      "* val_loss       : 9.5616e-01\n",
      "* val_error      : 5.8801e-01\n",
      "\n",
      "[ Epoch 20 / 300 ]\n",
      "* train_loss     : 1.5746e+00\n",
      "* train_error    : 5.8920e-01\n",
      "* val_loss       : 9.5347e-01\n",
      "* val_error      : 5.8718e-01\n",
      "\n",
      "[ Epoch 30 / 300 ]\n",
      "* train_loss     : 1.5661e+00\n",
      "* train_error    : 5.8892e-01\n",
      "* val_loss       : 9.5322e-01\n",
      "* val_error      : 5.8732e-01\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     35\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 36\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m train_epoch_loss \u001b[38;5;241m=\u001b[39m train_epoch_loss \u001b[38;5;241m+\u001b[39m train_loss \u001b[38;5;241m*\u001b[39m num_data\n\u001b[0;32m     39\u001b[0m train_pred  \u001b[38;5;241m=\u001b[39m normalizer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(train_pred)\n",
      "File \u001b[1;32mc:\\Users\\GJ2\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\GJ2\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\GJ2\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\GJ2\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GJ2\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\adam.py:353\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[0;32m    351\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mweight_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[0;32m    354\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_history = {\n",
    "    'train_loss':   [],\n",
    "    'train_error':  [],\n",
    "    'val_loss':     [],\n",
    "    'val_error':    [],\n",
    "    'train_time':   0.0,\n",
    "}\n",
    "normalizer['sol'].to(DEVICE)\n",
    "\n",
    "elapsed_time = time.time()\n",
    "for epoch in tqdm(range(1, NUM_EPOCHS + 1)):\n",
    "    # NOTE: Train\n",
    "    deeponet.train()\n",
    "    _train_time = time.time()\n",
    "    train_epoch_loss:  torch.Tensor = 0\n",
    "    train_epoch_error: torch.Tensor = 0\n",
    "    \n",
    "    grid: torch.Tensor; data: torch.Tensor; Kdata: torch.Tensor; Kdata_grad: torch.Tensor; target: torch.Tensor\n",
    "    for grid, data, Kdata, Kdata_grad, target in train_loader:\n",
    "        num_data = len(data)\n",
    "        \n",
    "        train_pred      = deeponet.forward((data, grid))\n",
    "        train_loss_data = criterion.forward(train_pred, target)\n",
    "        \n",
    "        grid = grid.clone()\n",
    "        grid.requires_grad_()\n",
    "        _train_pred_gov = deeponet.forward((data, grid))\n",
    "        # train_loss_gov  = 0\n",
    "        train_loss_gov  = compute_loss_gov(grid, _train_pred_gov, Kdata, Kdata_grad)\n",
    "        # assert False\n",
    "        train_loss_bc   = 0\n",
    "        train_loss      = train_loss_data + train_loss_gov + train_loss_bc\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss = train_epoch_loss + train_loss * num_data\n",
    "        train_pred  = normalizer['sol'].decode(train_pred)\n",
    "        target      = normalizer['sol'].decode(target)\n",
    "        train_epoch_error = train_epoch_error + (\n",
    "            torch.linalg.norm(train_pred - target) / (1e-8 + torch.linalg.norm(target))\n",
    "        ) * num_data\n",
    "    _train_time = time.time() - _train_time\n",
    "    train_history['train_time'] += _train_time\n",
    "    train_epoch_loss    = train_epoch_loss / TRAIN_SIZE\n",
    "    train_epoch_error   = train_epoch_error / TRAIN_SIZE\n",
    "    train_history['train_loss'].append(train_epoch_loss.item())\n",
    "    train_history['train_error'].append(train_epoch_error.item())\n",
    "    \n",
    "    \n",
    "    # NOTE: Validation\n",
    "    deeponet.eval()\n",
    "    val_epoch_loss:     torch.Tensor = 0\n",
    "    val_epoch_error:    torch.Tensor = 0\n",
    "    with torch.no_grad():\n",
    "        for grid, data, Kdata, Kdata_grad, target in val_loader:\n",
    "            num_data = len(data)\n",
    "            \n",
    "            val_pred = deeponet.forward((data, grid))\n",
    "            val_loss_data   = criterion.forward(val_pred, target)\n",
    "            val_loss_gov    = 0     # compute_loss_gov(grid, val_pred, Kdata, Kdata_grad)\n",
    "            val_loss_bc     = 0\n",
    "            val_loss = val_loss_data + val_loss_gov + val_loss_bc\n",
    "            \n",
    "            val_epoch_loss  = val_epoch_loss + val_loss * num_data\n",
    "            val_pred = normalizer['sol'].decode(val_pred)\n",
    "            target   = normalizer['sol'].decode(target)\n",
    "            val_epoch_error     = val_epoch_error + (\n",
    "                                        torch.linalg.norm(val_pred - target) / (1e-8 + torch.linalg.norm(target))\n",
    "                                    ) * num_data\n",
    "    val_epoch_loss      = val_epoch_loss / VAL_SIZE\n",
    "    val_epoch_error     = val_epoch_error / VAL_SIZE\n",
    "    train_history['val_loss'].append(val_epoch_loss.item())\n",
    "    train_history['val_error'].append(val_epoch_error.item())\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"[ Epoch {epoch} / {NUM_EPOCHS} ]\")\n",
    "        for k in train_history.keys():\n",
    "            if k == \"train_time\":\n",
    "                continue\n",
    "            print(f\"* {k:15s}: {train_history[k][-1]:.4e}\")\n",
    "        print()\n",
    "    \n",
    "elapsed_time = time.time() - elapsed_time\n",
    "print(f\"Elapsed time: {int(elapsed_time)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(5, dtype = torch.float, requires_grad = True).reshape(-1, 1)\n",
    "b = torch.cat([a, a ** 2]).sum()\n",
    "# print(b)\n",
    "compute_grad(b, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Save the model and the train history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeponet.cpu()\n",
    "\n",
    "# Save the model\n",
    "os.makedirs(f\"./{time_str}\", exist_ok = True)\n",
    "torch.save(deeponet.state_dict(), f\"{time_str}/physics_informed_deeponet_darcy{RESOLUTION}_res{GRID}.pth\")\n",
    "\n",
    "# Save the normalizer, which will also be used in prediction\n",
    "normalizer['sol'].cpu()\n",
    "torch.save(normalizer, f\"{time_str}/physics_informed_deeponet_darcy{RESOLUTION}_res{GRID}_normalizer.pth\")\n",
    "\n",
    "# Save the history\n",
    "with open(f\"{time_str}/physics_informed_deeponet_darcy{RESOLUTION}_res{GRID}.pickle\", \"wb\") as f:\n",
    "    pickle.dump(train_history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
